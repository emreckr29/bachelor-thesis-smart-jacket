{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d845b5e3-c80e-4600-910a-fb1d27c9ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/envs/python313/lib/python3.13/site-packages (2.3.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python313/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/python313/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/envs/python313/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python313/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in /opt/conda/envs/python313/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: pyquaternion in /opt/conda/envs/python313/lib/python3.13/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/python313/lib/python3.1/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/python313/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python313/lib/python3.1/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/python313/lib/python3.1/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/conda/envs/python313/lib/python3.1/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/python313/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/python313/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python313/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python313/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/conda/envs/python313/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/envs/python313/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/envs/python313/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/envs/python313/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/python313/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/python313/lib/python3.1/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/python313/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn tensorflow matplotlib seaborn pyquaternion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f846b0e-6abe-4f21-a3ac-f72779225071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 20:03:04.789114: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-27 20:03:04.789611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-27 20:03:04.869764: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-27 20:03:06.344087: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-27 20:03:06.345749: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adım 2: Veri Yükleme ve Ön İşleme Başladı...\n",
      "Toplam 131 adet .pkl kaydı yüklendi.\n",
      "Adım 3: Tüm veriler 80 adımına eşitleniyor...\n",
      "Boyut Eşitleme Tamamlandı.\n",
      "\n",
      "Adım 4: Veriyi Derin Öğrenme Formatına Dönüştürme Başladı...\n",
      "Bulunan sınıflar (3 adet): ['RoundedBack' 'SupportHand' 'True']\n",
      "Veri şekli (X): (131, 80, 40)\n",
      "Etiket şekli (y): (131, 3)\n",
      "Veri Dönüştürme Tamamlandı.\n",
      "\n",
      "Adım 5: Veriyi Ayırma ve Ölçekleme Başladı...\n",
      "Eğitim verisi: (104, 80, 40), Test verisi: (27, 80, 40)\n",
      "Veri Ayırma ve Ölçekleme Tamamlandı.\n",
      "\n",
      "Adım 6: LSTM Modeli Oluşturuluyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 20:03:08.657300: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/opt/conda/envs/python313/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">26,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m26,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,835</span> (159.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m40,835\u001b[0m (159.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,643</span> (158.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m40,643\u001b[0m (158.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Oluşturma Tamamlandı.\n",
      "\n",
      "Adım 7: Model Eğitimi Başlıyor...\n",
      "Epoch 1/60\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3416 - loss: 1.8520"
     ]
    }
   ],
   "source": [
    "# Adım 1: Gerekli Kütüphaneleri Yükleme\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pyquaternion import Quaternion # Normalizasyon için\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- KONFİGÜRASYON ---\n",
    "# Verinin bulunduğu ana klasör\n",
    "DATA_FOLDER = '' \n",
    "# Hangi hareketi eğitmek istiyoruz?\n",
    "MOVEMENT_TYPE = ''\n",
    "# Tüm zaman serilerini eşitleyeceğimiz uzunluk (gördüğümüz 80-82'ye göre 80 ideal)\n",
    "TIMESTEPS = 80\n",
    "NUM_SENSORS = 10\n",
    "FEATURES_PER_SENSOR = 4 # w, x, y, z\n",
    "FEATURES = NUM_SENSORS * FEATURES_PER_SENSOR # Toplam 40 özellik\n",
    "USERS_TO_ANALYZE = [\"User-A\",\"User-B\", \"User-E\"]\n",
    "\n",
    "\n",
    "# --- Adım 2.1: Normalizasyon için Yardımcı Fonksiyon ---\n",
    "\n",
    "def normalize_sequence(sequence_data):\n",
    "    \"\"\"\n",
    "    (N, 40) şeklindeki bir NumPy dizisini ilk karesine göre normalize eder.\n",
    "    Sıralamanın (w, x, y, z) olduğunu varsayar.\n",
    "    \"\"\"\n",
    "    if not sequence_data.any(): # Tamamen sıfırsa dokunma\n",
    "        return sequence_data\n",
    "    \n",
    "    # 1. İlk kareyi (referans yönelimi) al\n",
    "    first_frame = sequence_data[0] # (40,) şeklinde\n",
    "    inverse_references = []\n",
    "    \n",
    "    # 2. 10 sensör için de ters (inverse) quaternion'ları hesapla\n",
    "    for i in range(NUM_SENSORS):\n",
    "        offset = i * FEATURES_PER_SENSOR\n",
    "        w, x, y, z = first_frame[offset], first_frame[offset+1], first_frame[offset+2], first_frame[offset+3]\n",
    "        \n",
    "        # Olası bir sıfır veriye karşı koruma\n",
    "        if w == 0 and x == 0 and y == 0 and z == 0:\n",
    "             q_ref = Quaternion(1, 0, 0, 0) # Nötr (identity) quaternion\n",
    "        else:\n",
    "            q_ref = Quaternion(w, x, y, z)\n",
    "            \n",
    "        inverse_references.append(q_ref.inverse)\n",
    "        \n",
    "    normalized_sequence = []\n",
    "    \n",
    "    # 3. Tüm zaman adımlarını (timesteps) bu referanslara göre normalize et\n",
    "    for frame in sequence_data:\n",
    "        normalized_frame_features = []\n",
    "        for i in range(NUM_SENSORS):\n",
    "            offset = i * FEATURES_PER_SENSOR\n",
    "            w, x, y, z = frame[offset], frame[offset+1], frame[offset+2], frame[offset+3]\n",
    "            \n",
    "            if w == 0 and x == 0 and y == 0 and z == 0:\n",
    "                q_live = Quaternion(1, 0, 0, 0)\n",
    "            else:\n",
    "                q_live = Quaternion(w, x, y, z)\n",
    "            \n",
    "            # Referans yönelimi çıkar\n",
    "            q_normalized = inverse_references[i] * q_live\n",
    "            \n",
    "            # Yeni (w,x,y,z) değerlerini düz listeye ekle\n",
    "            normalized_frame_features.extend([q_normalized.w, q_normalized.x, q_normalized.y, q_normalized.z])\n",
    "        \n",
    "        normalized_sequence.append(normalized_frame_features)\n",
    "        \n",
    "    return np.array(normalized_sequence)\n",
    "\n",
    "\n",
    "# --- Adım 2: Veriyi Yükleme ve Ön İşleme ---\n",
    "print(\"Adım 2: Veri Yükleme ve Ön İşleme Başladı...\")\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "movement_path = os.path.join(DATA_FOLDER, MOVEMENT_TYPE)\n",
    "# User-A, User-B, ... klasörlerinde gezin\n",
    "for user_folder in USERS_TO_ANALYZE:\n",
    "    user_path = os.path.join(movement_path, user_folder)\n",
    "    if os.path.isdir(user_path) and user_folder.startswith('User-'):\n",
    "\n",
    "        # True, WrongElbow, ... klasörlerinde gezin\n",
    "        for label_folder in os.listdir(user_path):\n",
    "            label_path = os.path.join(user_path, label_folder)\n",
    "            if os.path.isdir(label_path):\n",
    "\n",
    "                # .pkl dosyalarını oku\n",
    "                for pkl_file in os.listdir(label_path):\n",
    "                    if pkl_file.endswith('.pkl'):\n",
    "                        file_path = os.path.join(label_path, pkl_file)\n",
    "\n",
    "                        try:\n",
    "                            with open(file_path, 'rb') as f:\n",
    "                                obj = pickle.load(f)\n",
    "\n",
    "                            # .pkl içindeki 'data' anahtarından veriyi al\n",
    "                            recording_data = np.array(obj['data'])\n",
    "\n",
    "                            # Veri geçerli mi diye kontrol et (en az 1 frame olmalı)\n",
    "                            if recording_data.shape[0] < 1 or recording_data.shape[1] != FEATURES:\n",
    "                                print(f\"Uyarı: Hatalı veri atlanıyor (şekil {recording_data.shape}): {file_path}\")\n",
    "                                continue\n",
    "\n",
    "                            # --- YENİ: Normalizasyon ---\n",
    "                            recording_data_normalized = normalize_sequence(recording_data)\n",
    "\n",
    "                            all_data.append(recording_data_normalized)\n",
    "                            all_labels.append(label_folder) # Etiket olarak klasör adını kullan\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Hata: {file_path} dosyası okunurken hata oluştu: {e}\")\n",
    "\n",
    "print(f\"Toplam {len(all_data)} adet .pkl kaydı yüklendi.\")\n",
    "\n",
    "# --- Adım 3: Boyut Eşitleme (Padding/Trimming) ---\n",
    "print(f\"Adım 3: Tüm veriler {TIMESTEPS} adımına eşitleniyor...\")\n",
    "\n",
    "# Keras'ın 'pad_sequences' fonksiyonunu kullanalım. Çok daha verimli.\n",
    "# 'post' = eksik veriyi sona ekle/sondan kırp\n",
    "X_padded = pad_sequences(\n",
    "    all_data, \n",
    "    maxlen=TIMESTEPS, \n",
    "    dtype='float32', \n",
    "    padding='post', \n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "print(\"Boyut Eşitleme Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 4: Veriyi Derin Öğrenme Formatına Dönüştürme ---\n",
    "print(\"Adım 4: Veriyi Derin Öğrenme Formatına Dönüştürme Başladı...\")\n",
    "\n",
    "X = X_padded # Artık tüm verilerimiz (sample_sayısı, 80, 40) şeklinde\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# Etiketleri sayısal değerlere çevir (örn: 'Correct' -> 0, 'WrongElbow' -> 1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Sınıf sayısını al\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Bulunan sınıflar ({num_classes} adet): {label_encoder.classes_}\")\n",
    "\n",
    "# Sayısal etiketleri one-hot encoding formatına çevir\n",
    "# Örn: 3 sınıf varsa, 1 -> [0, 1, 0]\n",
    "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "print(f\"Veri şekli (X): {X.shape}\")\n",
    "print(f\"Etiket şekli (y): {y_one_hot.shape}\")\n",
    "print(\"Veri Dönüştürme Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 5: Eğitim ve Test Verisi Olarak Ayırma ve Ölçekleme ---\n",
    "print(\"Adım 5: Veriyi Ayırma ve Ölçekleme Başladı...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_one_hot, \n",
    "    test_size=0.2,       # Verinin %20'sini test için ayır\n",
    "    random_state=42,     # Tekrarlanabilir sonuçlar için\n",
    "    stratify=y_one_hot   # Sınıf dağılımını koru (çok önemli)\n",
    ")\n",
    "\n",
    "# Ölçekleme (Scaling) için veriyi 2D'ye çevirmemiz gerekiyor\n",
    "# (sample_sayısı, 80, 40) -> (sample_sayısı * 80, 40)\n",
    "scaler = StandardScaler()\n",
    "X_train_reshaped = X_train.reshape(-1, FEATURES)\n",
    "scaler.fit(X_train_reshaped)\n",
    "\n",
    "X_train_scaled_reshaped = scaler.transform(X_train_reshaped)\n",
    "X_test_scaled_reshaped = scaler.transform(X_test.reshape(-1, FEATURES))\n",
    "\n",
    "# Veriyi tekrar 3D (LSTM formatı) haline getirelim\n",
    "X_train_scaled = X_train_scaled_reshaped.reshape(X_train.shape)\n",
    "X_test_scaled = X_test_scaled_reshaped.reshape(X_test.shape)\n",
    "\n",
    "print(f\"Eğitim verisi: {X_train_scaled.shape}, Test verisi: {X_test_scaled.shape}\")\n",
    "print(\"Veri Ayırma ve Ölçekleme Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 6: LSTM Modelini Oluşturma ---\n",
    "print(\"Adım 6: LSTM Modeli Oluşturuluyor...\")\n",
    "\n",
    "model = Sequential([\n",
    "    # Input katmanı: (80, 40) şeklinde veri alacak\n",
    "    LSTM(64, input_shape=(TIMESTEPS, FEATURES), return_sequences=True),\n",
    "    BatchNormalization(), # Katmanlar arası normalizasyon, öğrenmeyi hızlandırır\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(32, return_sequences=False), # Son LSTM katmanı, sadece son çıktıyı verir\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    # Çıkış katmanı: Sınıf sayısı kadar nöron ve 'softmax' aktivasyonu\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(\"Model Oluşturma Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 7: Modeli Eğitme ---\n",
    "print(\"Adım 7: Model Eğitimi Başlıyor...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    epochs=60, # Epoch sayısını 50-100 arası deneyebilirsin\n",
    "    batch_size=16, \n",
    "    validation_data=(X_test_scaled, y_test)\n",
    ")\n",
    "\n",
    "print(\"Model Eğitimi Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 8: Modeli Değerlendirme ---\n",
    "print(\"Adım 8: Model Değerlendirmesi...\")\n",
    "\n",
    "# Test verisi üzerindeki loss ve accuracy değerleri\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Tahminleri yap\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1) # Olasılıklardan en yüksek sınıfı seç\n",
    "y_test_labels = np.argmax(y_test, axis=1) # One-hot'tan normal etikete dön\n",
    "\n",
    "# Sınıflandırma Raporu (Precision, Recall, F1-score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Karışıklık Matrisi (Confusion Matrix)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues', \n",
    "    xticklabels=label_encoder.classes_, \n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.xlabel('Tahmin Edilen (Predicted)')\n",
    "plt.ylabel('Gerçek (True)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Adım 9: Modeli ve Gerekli Nesneleri Kaydetme ---\n",
    "print(\"\\nAdım 9: Model ve diğer nesneler kaydediliyor...\")\n",
    "\n",
    "MODEL_OUTPUT_DIR = f\"final_lstm_models_{MOVEMENT_TYPE}\"\n",
    "if not os.path.exists(MODEL_OUTPUT_DIR):\n",
    "    os.makedirs(MODEL_OUTPUT_DIR)\n",
    "\n",
    "# 1. Keras modelini kaydet\n",
    "model.save(os.path.join(MODEL_OUTPUT_DIR, f'{MOVEMENT_TYPE}_lstm_model.h5'))\n",
    "\n",
    "# 2. Scaler'ı kaydet\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, f'{MOVEMENT_TYPE}_scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# 3. Label Encoder'ı kaydet\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, f'{MOVEMENT_TYPE}_label_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(f\"Tüm dosyalar başarıyla '{MODEL_OUTPUT_DIR}' klasörüne kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19115a2-ce09-456d-bbe4-76c5562facf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 1: Gerekli Kütüphaneleri Yükleme\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pyquaternion import Quaternion # Normalizasyon için\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import GRU # <-- Değiştir\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- KONFİGÜRASYON ---\n",
    "# Verinin bulunduğu ana klasör\n",
    "DATA_FOLDER = '' \n",
    "# Hangi hareketi eğitmek istiyoruz?\n",
    "MOVEMENT_TYPE = ''\n",
    "# Tüm zaman serilerini eşitleyeceğimiz uzunluk (gördüğümüz 80-82'ye göre 80 ideal)\n",
    "TIMESTEPS = 80\n",
    "NUM_SENSORS = 10\n",
    "FEATURES_PER_SENSOR = 4 # w, x, y, z\n",
    "FEATURES = NUM_SENSORS * FEATURES_PER_SENSOR # Toplam 40 özellik\n",
    "USERS_TO_ANALYZE = [\"User-A\",\"User-B\", \"User-E\"]\n",
    "\n",
    "\n",
    "# --- Adım 2.1: Normalizasyon için Yardımcı Fonksiyon ---\n",
    "\n",
    "def normalize_sequence(sequence_data):\n",
    "    \"\"\"\n",
    "    (N, 40) şeklindeki bir NumPy dizisini ilk karesine göre normalize eder.\n",
    "    Sıralamanın (w, x, y, z) olduğunu varsayar.\n",
    "    \"\"\"\n",
    "    if not sequence_data.any(): # Tamamen sıfırsa dokunma\n",
    "        return sequence_data\n",
    "    \n",
    "    # 1. İlk kareyi (referans yönelimi) al\n",
    "    first_frame = sequence_data[0] # (40,) şeklinde\n",
    "    inverse_references = []\n",
    "    \n",
    "    # 2. 10 sensör için de ters (inverse) quaternion'ları hesapla\n",
    "    for i in range(NUM_SENSORS):\n",
    "        offset = i * FEATURES_PER_SENSOR\n",
    "        w, x, y, z = first_frame[offset], first_frame[offset+1], first_frame[offset+2], first_frame[offset+3]\n",
    "        \n",
    "        # Olası bir sıfır veriye karşı koruma\n",
    "        if w == 0 and x == 0 and y == 0 and z == 0:\n",
    "             q_ref = Quaternion(1, 0, 0, 0) # Nötr (identity) quaternion\n",
    "        else:\n",
    "            q_ref = Quaternion(w, x, y, z)\n",
    "            \n",
    "        inverse_references.append(q_ref.inverse)\n",
    "        \n",
    "    normalized_sequence = []\n",
    "    \n",
    "    # 3. Tüm zaman adımlarını (timesteps) bu referanslara göre normalize et\n",
    "    for frame in sequence_data:\n",
    "        normalized_frame_features = []\n",
    "        for i in range(NUM_SENSORS):\n",
    "            offset = i * FEATURES_PER_SENSOR\n",
    "            w, x, y, z = frame[offset], frame[offset+1], frame[offset+2], frame[offset+3]\n",
    "            \n",
    "            if w == 0 and x == 0 and y == 0 and z == 0:\n",
    "                q_live = Quaternion(1, 0, 0, 0)\n",
    "            else:\n",
    "                q_live = Quaternion(w, x, y, z)\n",
    "            \n",
    "            # Referans yönelimi çıkar\n",
    "            q_normalized = inverse_references[i] * q_live\n",
    "            \n",
    "            # Yeni (w,x,y,z) değerlerini düz listeye ekle\n",
    "            normalized_frame_features.extend([q_normalized.w, q_normalized.x, q_normalized.y, q_normalized.z])\n",
    "        \n",
    "        normalized_sequence.append(normalized_frame_features)\n",
    "        \n",
    "    return np.array(normalized_sequence)\n",
    "\n",
    "\n",
    "# --- Adım 2: Veriyi Yükleme ve Ön İşleme ---\n",
    "print(\"Adım 2: Veri Yükleme ve Ön İşleme Başladı...\")\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "movement_path = os.path.join(DATA_FOLDER, MOVEMENT_TYPE)\n",
    "# User-A, User-B, ... klasörlerinde gezin\n",
    "for user_folder in USERS_TO_ANALYZE:\n",
    "    user_path = os.path.join(movement_path, user_folder)\n",
    "    if os.path.isdir(user_path) and user_folder.startswith('User-'):\n",
    "\n",
    "        # True, WrongElbow, ... klasörlerinde gezin\n",
    "        for label_folder in os.listdir(user_path):\n",
    "            label_path = os.path.join(user_path, label_folder)\n",
    "            if os.path.isdir(label_path):\n",
    "\n",
    "                # .pkl dosyalarını oku\n",
    "                for pkl_file in os.listdir(label_path):\n",
    "                    if pkl_file.endswith('.pkl'):\n",
    "                        file_path = os.path.join(label_path, pkl_file)\n",
    "\n",
    "                        try:\n",
    "                            with open(file_path, 'rb') as f:\n",
    "                                obj = pickle.load(f)\n",
    "\n",
    "                            # .pkl içindeki 'data' anahtarından veriyi al\n",
    "                            recording_data = np.array(obj['data'])\n",
    "\n",
    "                            # Veri geçerli mi diye kontrol et (en az 1 frame olmalı)\n",
    "                            if recording_data.shape[0] < 1 or recording_data.shape[1] != FEATURES:\n",
    "                                print(f\"Uyarı: Hatalı veri atlanıyor (şekil {recording_data.shape}): {file_path}\")\n",
    "                                continue\n",
    "\n",
    "                            # --- YENİ: Normalizasyon ---\n",
    "                            recording_data_normalized = normalize_sequence(recording_data)\n",
    "\n",
    "                            all_data.append(recording_data_normalized)\n",
    "                            all_labels.append(label_folder) # Etiket olarak klasör adını kullan\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Hata: {file_path} dosyası okunurken hata oluştu: {e}\")\n",
    "\n",
    "print(f\"Toplam {len(all_data)} adet .pkl kaydı yüklendi.\")\n",
    "\n",
    "# --- Adım 3: Boyut Eşitleme (Padding/Trimming) ---\n",
    "print(f\"Adım 3: Tüm veriler {TIMESTEPS} adımına eşitleniyor...\")\n",
    "\n",
    "# Keras'ın 'pad_sequences' fonksiyonunu kullanalım. Çok daha verimli.\n",
    "# 'post' = eksik veriyi sona ekle/sondan kırp\n",
    "X_padded = pad_sequences(\n",
    "    all_data, \n",
    "    maxlen=TIMESTEPS, \n",
    "    dtype='float32', \n",
    "    padding='post', \n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "print(\"Boyut Eşitleme Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 4: Veriyi Derin Öğrenme Formatına Dönüştürme ---\n",
    "print(\"Adım 4: Veriyi Derin Öğrenme Formatına Dönüştürme Başladı...\")\n",
    "\n",
    "X = X_padded # Artık tüm verilerimiz (sample_sayısı, 80, 40) şeklinde\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# Etiketleri sayısal değerlere çevir (örn: 'Correct' -> 0, 'WrongElbow' -> 1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Sınıf sayısını al\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Bulunan sınıflar ({num_classes} adet): {label_encoder.classes_}\")\n",
    "\n",
    "# Sayısal etiketleri one-hot encoding formatına çevir\n",
    "# Örn: 3 sınıf varsa, 1 -> [0, 1, 0]\n",
    "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "print(f\"Veri şekli (X): {X.shape}\")\n",
    "print(f\"Etiket şekli (y): {y_one_hot.shape}\")\n",
    "print(\"Veri Dönüştürme Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 5: Eğitim ve Test Verisi Olarak Ayırma ve Ölçekleme ---\n",
    "print(\"Adım 5: Veriyi Ayırma ve Ölçekleme Başladı...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_one_hot, \n",
    "    test_size=0.2,       # Verinin %20'sini test için ayır\n",
    "    random_state=42,     # Tekrarlanabilir sonuçlar için\n",
    "    stratify=y_one_hot   # Sınıf dağılımını koru (çok önemli)\n",
    ")\n",
    "\n",
    "# Ölçekleme (Scaling) için veriyi 2D'ye çevirmemiz gerekiyor\n",
    "# (sample_sayısı, 80, 40) -> (sample_sayısı * 80, 40)\n",
    "scaler = StandardScaler()\n",
    "X_train_reshaped = X_train.reshape(-1, FEATURES)\n",
    "scaler.fit(X_train_reshaped)\n",
    "\n",
    "X_train_scaled_reshaped = scaler.transform(X_train_reshaped)\n",
    "X_test_scaled_reshaped = scaler.transform(X_test.reshape(-1, FEATURES))\n",
    "\n",
    "# Veriyi tekrar 3D (LSTM formatı) haline getirelim\n",
    "X_train_scaled = X_train_scaled_reshaped.reshape(X_train.shape)\n",
    "X_test_scaled = X_test_scaled_reshaped.reshape(X_test.shape)\n",
    "\n",
    "print(f\"Eğitim verisi: {X_train_scaled.shape}, Test verisi: {X_test_scaled.shape}\")\n",
    "print(\"Veri Ayırma ve Ölçekleme Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 6: LSTM Modelini Oluşturma ---\n",
    "print(\"Adım 6: LSTM Modeli Oluşturuluyor...\")\n",
    "\n",
    "model = Sequential([\n",
    "    # Input katmanı: (80, 40) şeklinde veri alacak\n",
    "    GRU(64, input_shape=(TIMESTEPS, FEATURES), return_sequences=True),\n",
    "    BatchNormalization(), # Katmanlar arası normalizasyon, öğrenmeyi hızlandırır\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    GRU(32, return_sequences=False), # Son LSTM katmanı, sadece son çıktıyı verir\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    # Çıkış katmanı: Sınıf sayısı kadar nöron ve 'softmax' aktivasyonu\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(\"Model Oluşturma Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 7: Modeli Eğitme ---\n",
    "print(\"Adım 7: Model Eğitimi Başlıyor...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    epochs=60, # Epoch sayısını 50-100 arası deneyebilirsin\n",
    "    batch_size=16, \n",
    "    validation_data=(X_test_scaled, y_test)\n",
    ")\n",
    "\n",
    "print(\"Model Eğitimi Tamamlandı.\\n\")\n",
    "\n",
    "\n",
    "# --- Adım 8: Modeli Değerlendirme ---\n",
    "print(\"Adım 8: Model Değerlendirmesi...\")\n",
    "\n",
    "# Test verisi üzerindeki loss ve accuracy değerleri\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Tahminleri yap\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1) # Olasılıklardan en yüksek sınıfı seç\n",
    "y_test_labels = np.argmax(y_test, axis=1) # One-hot'tan normal etikete dön\n",
    "\n",
    "# Sınıflandırma Raporu (Precision, Recall, F1-score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Karışıklık Matrisi (Confusion Matrix)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues', \n",
    "    xticklabels=label_encoder.classes_, \n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.xlabel('Tahmin Edilen (Predicted)')\n",
    "plt.ylabel('Gerçek (True)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Adım 9: Modeli ve Gerekli Nesneleri Kaydetme ---\n",
    "print(\"\\nAdım 9: Model ve diğer nesneler kaydediliyor...\")\n",
    "\n",
    "MODEL_OUTPUT_DIR = f\"final_lstm_models_{MOVEMENT_TYPE}\"\n",
    "if not os.path.exists(MODEL_OUTPUT_DIR):\n",
    "    os.makedirs(MODEL_OUTPUT_DIR)\n",
    "\n",
    "# 1. Keras modelini kaydet\n",
    "model.save(os.path.join(MODEL_OUTPUT_DIR, f'{MOVEMENT_TYPE}_lstm_model.h5'))\n",
    "\n",
    "# 2. Scaler'ı kaydet\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, f'{MOVEMENT_TYPE}_scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# 3. Label Encoder'ı kaydet\n",
    "with open(os.path.join(MODEL_OUTPUT_DIR, f'{MOVEMENT_TYPE}_label_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(f\"Tüm dosyalar başarıyla '{MODEL_OUTPUT_DIR}' klasörüne kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989b5ba5-2a12-45b5-a3c2-62df3da0e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adım 4: Veri Yükleme ve Ön İşleme Başladı...\n",
      "Kullanıcı yükleniyor: User-A\n",
      "Kullanıcı yükleniyor: User-B\n",
      "Kullanıcı yükleniyor: User-E\n",
      "Veri Yükleme Tamamlandı.\n",
      "\n",
      "Bulunan sınıflar (3 adet): ['RoundedBack' 'SupportHand' 'True']\n",
      "\n",
      "Adım 6: LOUOCV Test Çatısı Başlatılıyor...\n",
      "\n",
      "==============================================\n",
      "MODEL TEST EDİLİYOR: CNN-LSTM\n",
      "==============================================\n",
      "\n",
      "--- Test Kullanıcısı: User-A ---\n",
      "Eğitim verisi: (258, 80, 40), Test verisi: (45, 80, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python313/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-A için Doğruluk: 51.11%\n",
      "\n",
      "--- Test Kullanıcısı: User-B ---\n",
      "Eğitim verisi: (258, 80, 40), Test verisi: (45, 80, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python313/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-B için Doğruluk: 84.44%\n",
      "\n",
      "--- Test Kullanıcısı: User-E ---\n",
      "Eğitim verisi: (270, 80, 40), Test verisi: (41, 80, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python313/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-E için Doğruluk: 60.98%\n",
      "\n",
      "\n",
      "==============================================\n",
      "     LOUOCV GENEL SONUÇLARI     \n",
      "==============================================\n",
      "Test Edilen Hareket: \n",
      "Veri Seti: 3 Kullanıcı, Toplam 131 Orijinal Kayıt\n",
      "----------------------------------------------\n",
      "Model: CNN-LSTM     | Ortalama Genelleme Doğruluğu: 65.51%\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "# Adım 1: Gerekli Kütüphaneleri Yükleme\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy # Scaler'ı kopyalamak için\n",
    "from pyquaternion import Quaternion # Normalizasyon için\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow ve Keras kütüphanelerini import etme\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, GRU, Conv1D, MaxPooling1D, GlobalAveragePooling1D,\n",
    "    Dense, Dropout, BatchNormalization, Input, LayerNormalization, \n",
    "    MultiHeadAttention, Add\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Flatten # <-- Ekle\n",
    "\n",
    "\n",
    "# GPU hatalarını ve gereksiz uyarıları gizle\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# --- KONFİGÜRASYON ---\n",
    "DATA_FOLDER = '' \n",
    "MOVEMENT_TYPE = '' # Test etmek istediğin hareket\n",
    "TIMESTEPS = 80 # Veri setindeki (80, 40) şekline göre\n",
    "NUM_SENSORS = 10\n",
    "FEATURES_PER_SENSOR = 4 # w, x, y, z\n",
    "FEATURES = NUM_SENSORS * FEATURES_PER_SENSOR\n",
    "USERS_TO_ANALYZE = [\"User-A\", \"User-B\", \"User-E\"]\n",
    "\n",
    "# Eğitim Ayarları\n",
    "EPOCHS = 100 # Overfitting'i önlemek için EarlyStopping kullanacağız\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# --- Adım 2: Yardımcı Fonksiyonlar (Veri İşleme) ---\n",
    "\n",
    "def normalize_sequence(sequence_data):\n",
    "    \"\"\"\n",
    "    (N, 40) şeklindeki bir NumPy dizisini ilk karesine göre normalize eder.\n",
    "    (w, x, y, z) sırasına göre.\n",
    "    \"\"\"\n",
    "    if not sequence_data.any(): return sequence_data\n",
    "    first_frame = sequence_data[0]\n",
    "    inverse_references = []\n",
    "    for i in range(NUM_SENSORS):\n",
    "        offset = i * FEATURES_PER_SENSOR\n",
    "        w, x, y, z = first_frame[offset:offset+4]\n",
    "        q_ref = Quaternion(w, x, y, z) if (w or x or y or z) else Quaternion(1, 0, 0, 0)\n",
    "        inverse_references.append(q_ref.inverse)\n",
    "        \n",
    "    normalized_sequence = []\n",
    "    for frame in sequence_data:\n",
    "        normalized_frame_features = []\n",
    "        for i in range(NUM_SENSORS):\n",
    "            offset = i * FEATURES_PER_SENSOR\n",
    "            w, x, y, z = frame[offset:offset+4]\n",
    "            q_live = Quaternion(w, x, y, z) if (w or x or y or z) else Quaternion(1, 0, 0, 0)\n",
    "            q_normalized = inverse_references[i] * q_live\n",
    "            normalized_frame_features.extend([q_normalized.w, q_normalized.x, q_normalized.y, q_normalized.z])\n",
    "        normalized_sequence.append(normalized_frame_features)\n",
    "    return np.array(normalized_sequence)\n",
    "\n",
    "def augment_data(sequence_data, noise_level=0.01):\n",
    "    \"\"\"Veriye rastgele küçük gürültüler (jitter) ekler.\"\"\"\n",
    "    noise = np.random.normal(0, noise_level, sequence_data.shape)\n",
    "    return sequence_data + noise\n",
    "\n",
    "# --- Adım 3: Model Mimarilerini Oluşturan Fonksiyonlar ---\n",
    "\n",
    "INPUT_SHAPE = (TIMESTEPS, FEATURES)\n",
    "\n",
    "def build_cnn_lstm_model(num_classes):\n",
    "    \"\"\"Senin başarılı CNN-LSTM Hibrid modelin.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=\"CNN_LSTM\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn_ann_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Flatten(), # <-- GlobalAveragePooling yerine Flatten\n",
    "        \n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=\"CNN_ANN_Hybrid\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_pure_cnn_model(num_classes):\n",
    "    \"\"\"Sadece 1D-CNN katmanları kullanan model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling1D(), # <-- LSTM/GRU yerine\n",
    "        \n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=\"Pure_1D_CNN\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_transformer_model(num_classes):\n",
    "    \"\"\"Basit bir Transformer Encoder bloğu kullanan model.\"\"\"\n",
    "    inputs = Input(shape=INPUT_SHAPE)\n",
    "    \n",
    "    # 1. Önce CNN ile özellikleri çıkaralım (Transformer'a yardımcı olur)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # --- Transformer Encoder Bloğu ---\n",
    "    # 2. Multi-Head Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    attn_output = Dropout(0.2)(attn_output)\n",
    "    x = Add()([x, attn_output]) # Residual Connection (Toplama)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # 3. Feed Forward Kısmı\n",
    "    ff_output = Dense(128, activation='relu')(x)\n",
    "    ff_output = Dropout(0.2)(ff_output)\n",
    "    ff_output = Dense(64, activation='relu')(ff_output)\n",
    "    x = Add()([x, ff_output]) # Residual Connection\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    # --- Blok Sonu ---\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Transformer\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Adım 4: Veriyi Yükleme ve Ön İşleme ---\n",
    "print(\"Adım 4: Veri Yükleme ve Ön İşleme Başladı...\")\n",
    "data_by_user = {}\n",
    "all_labels_flat = []\n",
    "\n",
    "movement_path = os.path.join(DATA_FOLDER, MOVEMENT_TYPE)\n",
    "for user_folder in USERS_TO_ANALYZE:\n",
    "    user_path = os.path.join(movement_path, user_folder)\n",
    "    if not os.path.isdir(user_path):\n",
    "        print(f\"Uyarı: Kullanıcı klasörü atlanıyor (bulunamadı): {user_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Kullanıcı yükleniyor: {user_folder}\")\n",
    "    data_by_user[user_folder] = {'data': [], 'labels': []}\n",
    "    \n",
    "    for label_folder in os.listdir(user_path):\n",
    "        label_path = os.path.join(user_path, label_folder)\n",
    "        if os.path.isdir(label_path):\n",
    "            for pkl_file in os.listdir(label_path):\n",
    "                if pkl_file.endswith('.pkl'):\n",
    "                    file_path = os.path.join(label_path, pkl_file)\n",
    "                    try:\n",
    "                        with open(file_path, 'rb') as f:\n",
    "                            obj = pickle.load(f)\n",
    "                        recording_data = np.array(obj['data'])\n",
    "                        if recording_data.shape[0] < 1 or recording_data.shape[1] != FEATURES:\n",
    "                            continue\n",
    "                        \n",
    "                        # --- Normalizasyon ve Boyut Eşitleme ---\n",
    "                        recording_data_normalized = normalize_sequence(recording_data)\n",
    "                        recording_data_padded = pad_sequences(\n",
    "                            [recording_data_normalized], maxlen=TIMESTEPS, dtype='float32', \n",
    "                            padding='post', truncating='post'\n",
    "                        )[0] # [0] ile (1, 80, 40) yerine (80, 40) al\n",
    "                        \n",
    "                        data_by_user[user_folder]['data'].append(recording_data_padded)\n",
    "                        data_by_user[user_folder]['labels'].append(label_folder)\n",
    "                        all_labels_flat.append(label_folder)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Hata: {file_path} dosyası okunurken hata oluştu: {e}\")\n",
    "\n",
    "print(\"Veri Yükleme Tamamlandı.\\n\")\n",
    "\n",
    "# --- Adım 5: Etiket Kodlayıcıyı Hazırlama ---\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels_flat)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Bulunan sınıflar ({num_classes} adet): {label_encoder.classes_}\\n\")\n",
    "\n",
    "# --- Adım 6: LOUOCV Test Çatısı ---\n",
    "print(\"Adım 6: LOUOCV Test Çatısı Başlatılıyor...\")\n",
    "\n",
    "# Test edilecek modelleri ve oluşturma fonksiyonlarını bir sözlükte topla\n",
    "models_to_test = {\n",
    "    \"CNN-LSTM\": build_cnn_lstm_model,\n",
    "    #\"Pure 1D-CNN\": build_pure_cnn_model,\n",
    "    #\"Transformer\": build_transformer_model,\n",
    "    #\"ANN\": build_cnn_ann_model\n",
    "}\n",
    "\n",
    "# Sonuçları saklamak için\n",
    "overall_results = {}\n",
    "all_classification_reports = {}\n",
    "\n",
    "# Aşırı öğrenmeyi engellemek için Erken Durdurma\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Her bir model için döngü\n",
    "for model_name, build_fn in models_to_test.items():\n",
    "    print(f\"\\n==============================================\")\n",
    "    print(f\"MODEL TEST EDİLİYOR: {model_name}\")\n",
    "    print(f\"==============================================\")\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    # LOUOCV döngüsü (her kullanıcı için)\n",
    "    for user_to_test in USERS_TO_ANALYZE:\n",
    "        print(f\"\\n--- Test Kullanıcısı: {user_to_test} ---\")\n",
    "        \n",
    "        # 1. Eğitim ve Test Verisini Ayır\n",
    "        X_train_list, y_train_list = [], []\n",
    "        X_test_list, y_test_list = [], []\n",
    "\n",
    "        for user, data in data_by_user.items():\n",
    "            if user == user_to_test:\n",
    "                X_test_list.extend(data['data'])\n",
    "                y_test_list.extend(data['labels'])\n",
    "            else:\n",
    "                X_train_list.extend(data['data'])\n",
    "                y_train_list.extend(data['labels'])\n",
    "\n",
    "        # 2. Veri Artırma (Sadece Eğitim Verisine)\n",
    "        X_train_aug, y_train_aug = [], []\n",
    "        for i in range(len(X_train_list)):\n",
    "            seq, lbl = X_train_list[i], y_train_list[i]\n",
    "            X_train_aug.append(seq) # Orijinal\n",
    "            y_train_aug.append(lbl)\n",
    "            X_train_aug.append(augment_data(seq, 0.01)) # Gürültülü 1\n",
    "            y_train_aug.append(lbl)\n",
    "            X_train_aug.append(augment_data(seq, 0.005)) # Gürültülü 2\n",
    "            y_train_aug.append(lbl)\n",
    "\n",
    "        X_train = np.array(X_train_aug)\n",
    "        X_test = np.array(X_test_list)\n",
    "        \n",
    "        # 3. Etiketleri Kodla\n",
    "        y_train = to_categorical(label_encoder.transform(y_train_aug), num_classes=num_classes)\n",
    "        y_test = to_categorical(label_encoder.transform(y_test_list), num_classes=num_classes)\n",
    "\n",
    "        # 4. Ölçekle (Scaler'ı SADECE artırılmış eğitim verisine göre eğit)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_reshaped = X_train.reshape(-1, FEATURES)\n",
    "        scaler.fit(X_train_reshaped)\n",
    "        \n",
    "        X_train_scaled = scaler.transform(X_train_reshaped).reshape(X_train.shape)\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(-1, FEATURES)).reshape(X_test.shape)\n",
    "        \n",
    "        print(f\"Eğitim verisi: {X_train_scaled.shape}, Test verisi: {X_test_scaled.shape}\")\n",
    "\n",
    "        # 5. Modeli Oluştur ve Eğit\n",
    "        model = build_fn(num_classes)\n",
    "        model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_test_scaled, y_test),\n",
    "            callbacks=[early_stopping], # Erken durdurmayı kullan\n",
    "            verbose=0 # Logları gizle\n",
    "        )\n",
    "        \n",
    "        # 6. Değerlendir\n",
    "        loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "        fold_scores.append(accuracy)\n",
    "        print(f\"{user_to_test} için Doğruluk: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Modelin ortalama LOUOCV skorunu kaydet\n",
    "    overall_results[model_name] = np.mean(fold_scores)\n",
    "\n",
    "\n",
    "# --- Adım 7: Nihai Sonuçları Göster ---\n",
    "print(\"\\n\\n==============================================\")\n",
    "print(\"     LOUOCV GENEL SONUÇLARI     \")\n",
    "print(\"==============================================\")\n",
    "print(f\"Test Edilen Hareket: {MOVEMENT_TYPE}\")\n",
    "print(f\"Veri Seti: {len(USERS_TO_ANALYZE)} Kullanıcı, Toplam {len(all_labels_flat)} Orijinal Kayıt\")\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "for model_name, avg_accuracy in overall_results.items():\n",
    "    print(f\"Model: {model_name.ljust(12)} | Ortalama Genelleme Doğruluğu: {avg_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd503c7e-7d82-4da9-b4a6-8e42af8e178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adım 4: Veri Yükleniyor ve İşleniyor...\n",
      "Toplam 131 adet orijinal .pkl kaydı yüklendi.\n",
      "Bulunan sınıflar: ['RoundedBack' 'SupportHand' 'True']\n",
      "Eğitim verisi: (104, 80, 40), Test verisi: (27, 80, 40)\n",
      "Veri Ayırma ve Ölçekleme Tamamlandı.\n",
      "\n",
      "Adım 5: Karşılaştırmalı Model Testi Başlatılıyor...\n",
      "\n",
      "--- Model Eğitiliyor: CNN-LSTM ---\n",
      "Test Doğruluğu: 100.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " RoundedBack       1.00      1.00      1.00         9\n",
      " SupportHand       1.00      1.00      1.00         9\n",
      "        True       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        27\n",
      "   macro avg       1.00      1.00      1.00        27\n",
      "weighted avg       1.00      1.00      1.00        27\n",
      "\n",
      "\n",
      "--- Model Eğitiliyor: Pure 1D-CNN ---\n",
      "Test Doğruluğu: 88.89%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " RoundedBack       0.75      1.00      0.86         9\n",
      " SupportHand       1.00      1.00      1.00         9\n",
      "        True       1.00      0.67      0.80         9\n",
      "\n",
      "    accuracy                           0.89        27\n",
      "   macro avg       0.92      0.89      0.89        27\n",
      "weighted avg       0.92      0.89      0.89        27\n",
      "\n",
      "\n",
      "--- Model Eğitiliyor: Transformer ---\n",
      "Test Doğruluğu: 92.59%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " RoundedBack       1.00      0.78      0.88         9\n",
      " SupportHand       1.00      1.00      1.00         9\n",
      "        True       0.82      1.00      0.90         9\n",
      "\n",
      "    accuracy                           0.93        27\n",
      "   macro avg       0.94      0.93      0.92        27\n",
      "weighted avg       0.94      0.93      0.93        27\n",
      "\n",
      "\n",
      "--- Model Eğitiliyor: ANN ---\n",
      "Test Doğruluğu: 100.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " RoundedBack       1.00      1.00      1.00         9\n",
      " SupportHand       1.00      1.00      1.00         9\n",
      "        True       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        27\n",
      "   macro avg       1.00      1.00      1.00        27\n",
      "weighted avg       1.00      1.00      1.00        27\n",
      "\n",
      "\n",
      "\n",
      "==============================================\n",
      "     RASTGELE BÖLME (Mixed-User) TEST SONUÇLARI     \n",
      "==============================================\n",
      "Test Edilen Hareket: \n",
      "Veri Seti: 5 Kullanıcı, Toplam 131 Orijinal Kayıt\n",
      "Veri Artırma: KULLANILMADI\n",
      "----------------------------------------------\n",
      "Model: CNN-LSTM     | Kişiselleştirilmiş Doğruluk: 100.00%\n",
      "Model: Pure 1D-CNN  | Kişiselleştirilmiş Doğruluk: 88.89%\n",
      "Model: Transformer  | Kişiselleştirilmiş Doğruluk: 92.59%\n",
      "Model: ANN          | Kişiselleştirilmiş Doğruluk: 100.00%\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "# Adım 1: Gerekli Kütüphaneleri Yükleme\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pyquaternion import Quaternion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, GRU, Conv1D, MaxPooling1D, GlobalAveragePooling1D,\n",
    "    Dense, Dropout, BatchNormalization, Input, LayerNormalization, \n",
    "    MultiHeadAttention, Add\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "from tensorflow.keras.layers import Flatten # <-- Ekle\n",
    "# Gereksiz TensorFlow ve Keras uyarılarını gizle\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "tf.get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Keras uyarılarını gizle\n",
    "\n",
    "# --- KONFİGÜRASYON ---\n",
    "DATA_FOLDER = '' \n",
    "MOVEMENT_TYPE = ''\n",
    "TIMESTEPS = 80\n",
    "NUM_SENSORS = 10\n",
    "FEATURES_PER_SENSOR = 4 # w, x, y, z\n",
    "FEATURES = NUM_SENSORS * FEATURES_PER_SENSOR\n",
    "USERS_TO_ANALYZE = [\"User-A\", \"User-B\", \"User-C\", \"User-D\", \"User-E\"]\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# --- Adım 2: Yardımcı Fonksiyonlar ---\n",
    "\n",
    "def normalize_sequence(sequence_data):\n",
    "    \"\"\"(N, 40) veriyi ilk karesine göre normalize eder.\"\"\"\n",
    "    if not sequence_data.any(): return sequence_data\n",
    "    first_frame = sequence_data[0]\n",
    "    inverse_references = []\n",
    "    for i in range(NUM_SENSORS):\n",
    "        offset = i * FEATURES_PER_SENSOR\n",
    "        w, x, y, z = first_frame[offset:offset+4]\n",
    "        q_ref = Quaternion(w, x, y, z) if (w or x or y or z) else Quaternion(1, 0, 0, 0)\n",
    "        inverse_references.append(q_ref.inverse)\n",
    "        \n",
    "    normalized_sequence = []\n",
    "    for frame in sequence_data:\n",
    "        normalized_frame_features = []\n",
    "        for i in range(NUM_SENSORS):\n",
    "            offset = i * FEATURES_PER_SENSOR\n",
    "            w, x, y, z = frame[offset:offset+4]\n",
    "            q_live = Quaternion(w, x, y, z) if (w or x or y or z) else Quaternion(1, 0, 0, 0)\n",
    "            q_normalized = inverse_references[i] * q_live\n",
    "            normalized_frame_features.extend([q_normalized.w, q_normalized.x, q_normalized.y, q_normalized.z])\n",
    "        normalized_sequence.append(normalized_frame_features)\n",
    "    return np.array(normalized_sequence)\n",
    "\n",
    "# --- Adım 3: Model Mimarilerini Oluşturan Fonksiyonlar ---\n",
    "INPUT_SHAPE = (TIMESTEPS, FEATURES)\n",
    "\n",
    "def build_cnn_lstm_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=\"CNN_LSTM\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn_gru_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        GRU(50, return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=\"CNN_GRU\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn_ann_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Flatten(), # <-- GlobalAveragePooling yerine Flatten\n",
    "        \n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=\"CNN_ANN_Hybrid\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "def build_pure_cnn_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=\"Pure_1D_CNN\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_transformer_model(num_classes):\n",
    "    inputs = Input(shape=INPUT_SHAPE)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    attn_output = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    attn_output = Dropout(0.2)(attn_output)\n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    ff_output = Dense(128, activation='relu')(x)\n",
    "    ff_output = Dropout(0.2)(ff_output)\n",
    "    ff_output = Dense(64, activation='relu')(ff_output)\n",
    "    x = Add()([x, ff_output])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Transformer\")\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Adım 4: Veriyi Yükleme, İşleme ve Ayırma ---\n",
    "print(\"Adım 4: Veri Yükleniyor ve İşleniyor...\")\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "movement_path = os.path.join(DATA_FOLDER, MOVEMENT_TYPE)\n",
    "\n",
    "for user_folder in USERS_TO_ANALYZE:\n",
    "    user_path = os.path.join(movement_path, user_folder)\n",
    "    if not os.path.isdir(user_path): continue\n",
    "    for label_folder in os.listdir(user_path):\n",
    "        label_path = os.path.join(user_path, label_folder)\n",
    "        if os.path.isdir(label_path):\n",
    "            for pkl_file in os.listdir(label_path):\n",
    "                if pkl_file.endswith('.pkl'):\n",
    "                    file_path = os.path.join(label_path, pkl_file)\n",
    "                    try:\n",
    "                        with open(file_path, 'rb') as f: obj = pickle.load(f)\n",
    "                        recording_data = np.array(obj['data'])\n",
    "                        if recording_data.shape[0] < 1 or recording_data.shape[1] != FEATURES: continue\n",
    "                        \n",
    "                        recording_data_normalized = normalize_sequence(recording_data)\n",
    "                        recording_data_padded = pad_sequences(\n",
    "                            [recording_data_normalized], maxlen=TIMESTEPS, dtype='float32', \n",
    "                            padding='post', truncating='post'\n",
    "                        )[0]\n",
    "                        \n",
    "                        all_data.append(recording_data_padded)\n",
    "                        all_labels.append(label_folder)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Hata: {file_path} dosyası okunurken hata oluştu: {e}\")\n",
    "\n",
    "print(f\"Toplam {len(all_data)} adet orijinal .pkl kaydı yüklendi.\")\n",
    "\n",
    "# Etiketleri Kodla\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(all_labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
    "X = np.array(all_data)\n",
    "\n",
    "print(f\"Bulunan sınıflar: {label_encoder.classes_}\")\n",
    "\n",
    "# Veriyi Rastgele Ayır (LOUOCV OLMADAN)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_one_hot, \n",
    "    test_size=0.2, # 300 kaydın %20'si = 60 test kaydı\n",
    "    #random_state=42, \n",
    "    stratify=y_one_hot # Sınıf dağılımını koru\n",
    ")\n",
    "\n",
    "# Ölçekle\n",
    "scaler = StandardScaler()\n",
    "X_train_reshaped = X_train.reshape(-1, FEATURES)\n",
    "scaler.fit(X_train_reshaped)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_reshaped).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, FEATURES)).reshape(X_test.shape)\n",
    "\n",
    "print(f\"Eğitim verisi: {X_train_scaled.shape}, Test verisi: {X_test_scaled.shape}\")\n",
    "print(\"Veri Ayırma ve Ölçekleme Tamamlandı.\\n\")\n",
    "\n",
    "# --- Adım 5: Modelleri Karşılaştırmalı Olarak Eğitme ve Değerlendirme ---\n",
    "print(\"Adım 5: Karşılaştırmalı Model Testi Başlatılıyor...\")\n",
    "\n",
    "models_to_test = {\n",
    "    \"CNN-LSTM\": build_cnn_lstm_model,\n",
    "    \"Pure 1D-CNN\": build_pure_cnn_model,\n",
    "    \"Transformer\": build_transformer_model,\n",
    "    \"ANN\": build_cnn_ann_model,\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "results = {}\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "for model_name, build_fn in models_to_test.items():\n",
    "    print(f\"\\n--- Model Eğitiliyor: {model_name} ---\")\n",
    "    model = build_fn(num_classes)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0 # Logları gizle\n",
    "    )\n",
    "    \n",
    "    # Değerlendir\n",
    "    loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    print(f\"Test Doğruluğu: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    y_pred_probs = model.predict(X_test_scaled)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test_labels, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    results[model_name] = accuracy\n",
    "\n",
    "# --- Adım 6: Nihai Sonuçları Göster ---\n",
    "print(\"\\n\\n==============================================\")\n",
    "print(\"     RASTGELE BÖLME (Mixed-User) TEST SONUÇLARI     \")\n",
    "print(\"==============================================\")\n",
    "print(f\"Test Edilen Hareket: {MOVEMENT_TYPE}\")\n",
    "print(f\"Veri Seti: {len(USERS_TO_ANALYZE)} Kullanıcı, Toplam {len(all_labels)} Orijinal Kayıt\")\n",
    "print(\"Veri Artırma: KULLANILMADI\")\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "for model_name, avg_accuracy in results.items():\n",
    "    print(f\"Model: {model_name.ljust(12)} | Kişiselleştirilmiş Doğruluk: {avg_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5c6ee-c642-453b-b809-76a6008cb029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
